{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5512ebf1-e476-4a4a-b15e-c56ce8b4ae01",
   "metadata": {},
   "source": [
    "## Assembly Functions\n",
    "These are a collection of functions that will work together to take the work (and libraries and functions) made with the function and library management and apply it. The end product will use user input to create a file and populate it with functions and the libraries and functions associated with those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8f9dd1-9e18-4a0d-901a-af5814084c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base\n",
    "\n",
    "import pandas\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a40bd5-5354-4e89-8950-1f9b32df615c",
   "metadata": {},
   "source": [
    "### Part 1: Get module name\n",
    "\n",
    "**Big Idea:** Get user input to get the name of the file to be created\n",
    "\n",
    "**Final Product:** A function that can take user input to get the filename. It will check if the filename includes .py at the end and format the name appropriately if it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c05f792e-9d00-4370-b017-14d0dc524109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_filename():\n",
    "    filename = input(\"Enter the filename: \")\n",
    "\n",
    "    # Check if the filename ends with .py\n",
    "    if not filename.endswith('.py'):\n",
    "        filename += '.py'\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2393bc7c-bb0f-4a8d-b587-d16a1bf0dce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the filename:  wrangle.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'wrangle.py'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_filename()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5cc31-744b-4526-85a5-a8f1ebb6e279",
   "metadata": {},
   "source": [
    "### Part 2: Get functions to include\n",
    "\n",
    "**Big Idea:** Get functions from the user and return that list for usage elsewhere.\n",
    "\n",
    "**Final Product:** A function that displays a list of available functions for use. It will get a string from the user and parse that string into a list of functions (and drop any that don't exist). Then it will show that string to the user and prompt the user to start over or add more (or press enter if it's good). It will then return this list.\n",
    "\n",
    "*Optional:* Use the the module name to suggest functions that are associated with that module name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b47101-45dc-471a-860e-235c0e836dac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_functions():\n",
    "    # Retrieve the functions\n",
    "    df = base.data_saver(load_function=True)\n",
    "    \n",
    "    while True:\n",
    "        # Clear screen\n",
    "        time.sleep(0.05)\n",
    "        clear_output()\n",
    "        \n",
    "        # Display available functions\n",
    "        base.view_list(df)\n",
    "        \n",
    "        # Get user input\n",
    "        input_string = input(\"Enter function names separated by commas: \")\n",
    "        input_list = [name.strip() for name in input_string.split(',') if name.strip()]\n",
    "\n",
    "        # Filter against DataFrame\n",
    "        valid_functions = df[df['name'].isin(input_list)]['name'].tolist()\n",
    "\n",
    "        if not valid_functions:\n",
    "            print(\"None of the entered functions are available. Please try again.\")\n",
    "            time.sleep(1.5)\n",
    "            continue\n",
    "\n",
    "        print(\"Selected functions:\", ', '.join(valid_functions))\n",
    "        choice = input(\"Press Enter to confirm or 'restart' to start over: \").lower()\n",
    "\n",
    "        if choice == 'restart':\n",
    "            continue\n",
    "        else:\n",
    "            # Return the list of valid functions if user confirms\n",
    "            return valid_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "00028102-430b-44ef-8a46-9d83b07b1b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current list of entries:\n",
      "df_info: Function takes a dataframe and returns potentially relevant information about it\n",
      "check_file_exists: Generic function to check if a file exists\n",
      "drop_extras: Function to drop extra columns that may have a smaller impact on the model\n",
      "split_categorical: Returns three dataframes split from one for use in model training, validation, and testing\n",
      "drop_cols: Drops columns\n",
      "encode_df: Takes a processed dataframe and encodes the object columns for usage in modeling\n",
      "Xy_sets: Encodes and returns X_sets and y_sets\n",
      "test_hypothesis: Runs a quick statistical test and informs of rejection or failure to reject\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter function names separated by commas:  df_info, check_file_exists, drop_cols, Xy_sets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected functions: df_info, check_file_exists, drop_cols, Xy_sets\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to confirm or 'restart' to start over:  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['df_info', 'check_file_exists', 'drop_cols', 'Xy_sets']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc329bc7-e7c6-4ebe-b18d-c51cbabe32d2",
   "metadata": {},
   "source": [
    "### Part 3: Get function dependencies\n",
    "**Big Idea:** Use the dependencies field of the functions list to get any dependencies related to functions.\n",
    "\n",
    "**Final Product:** A function that takes a list containing the names of functions to include. It will then iterate through this list and get the names of any functions that are necessary to run the functions. It will do this recursively until there are no functions found. If no functions found, it returns none. Any returns (excepting none) are formatted as a list and appended to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b9bef8-05c7-495d-abf8-3c9c4d9ddd05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_dependencies(func_names, all_dependencies=set()):\n",
    "    df = base.data_saver(load_function=True)\n",
    "    \n",
    "    new_dependencies = set()\n",
    "\n",
    "    for func in func_names:\n",
    "        # Get dependencies for the current function\n",
    "        current_dependencies = df[df['name'] == func]['dependencies'].iloc[0]\n",
    "        \n",
    "        # Extract function dependencies if they exist\n",
    "        func_dependencies = current_dependencies.get('function', []) if isinstance(current_dependencies, dict) else []\n",
    "\n",
    "        # Add new dependencies to the set\n",
    "        for dependency in func_dependencies:\n",
    "            if dependency not in all_dependencies:\n",
    "                new_dependencies.add(dependency)\n",
    "\n",
    "    # Update the overall dependencies set\n",
    "    all_dependencies.update(new_dependencies)\n",
    "\n",
    "    # Recursively find dependencies for the new dependencies\n",
    "    if new_dependencies:\n",
    "        find_dependencies(new_dependencies, all_dependencies)\n",
    "\n",
    "    return list(all_dependencies) if all_dependencies else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aa280f63-f01e-4866-8136-92064327fcb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encode_df', 'drop_extras']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions = ['df_info', 'check_file_exists', 'drop_cols', 'Xy_sets']\n",
    "\n",
    "find_dependencies(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db011ede-80ee-4923-a963-9b469e0f4775",
   "metadata": {},
   "source": [
    "### Part 4: Get library dependencies\n",
    "**Big Idea:** Use the dependencies field of a functions list to get any dependencies related to libraries.\n",
    "\n",
    "**Final Product:** A function that takes a list containing the names of functions to include. It will then iterate through this list and get the names of any libraries necessary to run the functions. It will then return this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09bd4c10-e26b-4824-8623-f6f64d67649c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_library_dependencies(func_names):\n",
    "    df = base.data_saver(load_function=True)\n",
    "    required_libraries = set()\n",
    "\n",
    "    for func in func_names:\n",
    "        # Retrieve the dependency record for the function\n",
    "        dependency_record = df[df['name'] == func]['dependencies'].iloc[0]\n",
    "\n",
    "        # Check if the record is a dictionary and contains the 'library' key\n",
    "        if isinstance(dependency_record, dict) and 'library' in dependency_record:\n",
    "            libraries = dependency_record['library']\n",
    "            required_libraries.update(libraries)\n",
    "\n",
    "    return list(required_libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4283b0ec-d0e9-4782-950d-66d49d208986",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['os', 'pandas']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions = ['encode_df', 'drop_extras','df_info', 'check_file_exists', 'drop_cols', 'Xy_sets']\n",
    "\n",
    "find_library_dependencies(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cf1271f8-f938-467c-b391-84fbccaff47d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "      <th>tags</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>syntax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>df_info</td>\n",
       "      <td>Function takes a dataframe and returns potenti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'library': ['pandas']}</td>\n",
       "      <td>def df_info(df,include=False,samples=1):\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>check_file_exists</td>\n",
       "      <td>Generic function to check if a file exists</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'library': ['os', 'pandas']}</td>\n",
       "      <td>def check_file_exists(filename,query,url):\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drop_extras</td>\n",
       "      <td>Function to drop extra columns that may have a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'library': ['pandas']}</td>\n",
       "      <td>def drop_extras(df,target,degree=6):\\n     \"\"\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>split_categorical</td>\n",
       "      <td>Returns three dataframes split from one for us...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'library': ['pandas', 'train_test_split']}</td>\n",
       "      <td>def split_categorical(df,strat_var,seed=123):\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drop_cols</td>\n",
       "      <td>Drops columns</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'library': ['pandas'], 'function': ['drop_ext...</td>\n",
       "      <td>def drop_cols(df,cols=[],extras=False,degree=6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>encode_df</td>\n",
       "      <td>Takes a processed dataframe and encodes the ob...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'library': ['pandas']}</td>\n",
       "      <td>def encode_df(df,target):\\n     '''\\n     Take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Xy_sets</td>\n",
       "      <td>Encodes and returns X_sets and y_sets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'library': ['pandas'], 'function': ['encode_d...</td>\n",
       "      <td>def Xy_sets(tvt_set,target):\\n     '''\\n     E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_hypothesis</td>\n",
       "      <td>Runs a quick statistical test and informs of r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>def test_hypothesis(p,\\n                      ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                               desc  tags  \\\n",
       "0            df_info  Function takes a dataframe and returns potenti...   NaN   \n",
       "1  check_file_exists         Generic function to check if a file exists   NaN   \n",
       "2        drop_extras  Function to drop extra columns that may have a...   NaN   \n",
       "3  split_categorical  Returns three dataframes split from one for us...   NaN   \n",
       "4          drop_cols                                      Drops columns   NaN   \n",
       "5          encode_df  Takes a processed dataframe and encodes the ob...   NaN   \n",
       "6            Xy_sets              Encodes and returns X_sets and y_sets   NaN   \n",
       "7    test_hypothesis  Runs a quick statistical test and informs of r...   NaN   \n",
       "\n",
       "                                        dependencies  \\\n",
       "0                            {'library': ['pandas']}   \n",
       "1                      {'library': ['os', 'pandas']}   \n",
       "2                            {'library': ['pandas']}   \n",
       "3        {'library': ['pandas', 'train_test_split']}   \n",
       "4  {'library': ['pandas'], 'function': ['drop_ext...   \n",
       "5                            {'library': ['pandas']}   \n",
       "6  {'library': ['pandas'], 'function': ['encode_d...   \n",
       "7                                               None   \n",
       "\n",
       "                                              syntax  \n",
       "0  def df_info(df,include=False,samples=1):\\n    ...  \n",
       "1  def check_file_exists(filename,query,url):\\n  ...  \n",
       "2  def drop_extras(df,target,degree=6):\\n     \"\"\"...  \n",
       "3  def split_categorical(df,strat_var,seed=123):\\...  \n",
       "4  def drop_cols(df,cols=[],extras=False,degree=6...  \n",
       "5  def encode_df(df,target):\\n     '''\\n     Take...  \n",
       "6  def Xy_sets(tvt_set,target):\\n     '''\\n     E...  \n",
       "7  def test_hypothesis(p,\\n                      ...  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.data_saver(load_function=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e8362-3c74-49a3-a20d-aa586e52ab89",
   "metadata": {},
   "source": [
    "### Part 5: Syntax Grabber\n",
    "**Big Idea:** For a given name, return the syntax.\n",
    "\n",
    "**Final Product:** A function that can be given a list and a DataFrame, check the list against the DataFrame, and retrieve the syntax for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166056a9-ff83-483f-8051-e0ef75fb99ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def syntax_grabber(item_list, df):\n",
    "    syntax_list = []\n",
    "\n",
    "    for item in item_list:\n",
    "        # Find the row in the DataFrame where 'name' matches 'item'\n",
    "        matching_row = df[df['name'] == item]\n",
    "        \n",
    "        syntax_list.append(matching_row['syntax'].iloc[0])\n",
    "\n",
    "    return syntax_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f44663dc-13fc-419a-900c-1a3b3313792a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"def encode_df(df,target):\\n     '''\\n     Takes a processed dataframe and encodes the object columns for usage in modeling.\\n          Takes a dataframe and a target variable (assuming the target variable is an object). Target variable keeps the thing the model is being trained on from splitting and altering it.\\n          !!! MAKE ME MORE DYNAMIC !!!\\n     - Add functionality to check if passed a list or dataframe\\n     - If dataframe, then run standard loop\\n     - If list then check if each item is a dataframe (checking for train/validate/test)\\n     - If list and each item is dataframe, then try loop on each dataframe\\n     - Otherwise return an error\\n     '''\\n     # Get the object columns from the dataframe\\n     obj_col = [col for col in df.columns if df[col].dtype == 'O']\\n          # remove target variable\\n     obj_col.remove(target)\\n          # Begin encoding the object columns\\n     for col in obj_col:\\n         # Grab current column dummies\\n         dummies = pd.get_dummies(df[col],drop_first=True)\\n                  # concatenate the names in a descriptive manner\\n         dummies.columns = [col+'_is_'+column for column in dummies.columns]\\n          # add these new columns to the dataframe\\n         for column in dummies.columns:\\n             df[column] = dummies[column].astype(float)\\n                  # Drop the old columns from the dataframe\\n         df = df.drop(columns=col)\\n          return df \",\n",
       " 'def drop_extras(df,target,degree=6):\\n     \"\"\"\\n     Function to drop extra columns that may have a smaller impact on the model. Requires dataframe be cleaned first.\\n          Takes a DataFrame, and returns a DataFrame.\\n          Degree indicates the index of object columns to begin selecting for drop off.\\n         Hint: smaller value means drop more columns, larger value means drop fewer columns!\\n     \"\"\"\\n     from scipy import stats\\n          corr_dict = {}\\n     obj_cols = []\\n     alpha = 0.05\\n          # grab object columns from dataframe\\n     for col in df.columns:\\n         if df[col].dtype == \\'O\\':\\n             # print(f\\'{col}: object\\')\\n             obj_cols.append(col)\\n          # get p-values of columns\\n     for col in obj_cols:\\n         observed = pd.crosstab(df[target],df[col])\\n         # print(observed)\\n          t,p,dof,expected = stats.chi2_contingency(observed)\\n          # if p < alpha:\\n             # print(f\\'{col} has potential correlation with churn at {p}\\')\\n         corr_dict[col] = p\\n                       # grabs\\n      drop_extra = sorted(corr_dict, key=corr_dict.get)[degree:]\\n          df = df.drop(columns=drop_extra)\\n          return df',\n",
       " 'def df_info(df,include=False,samples=1):\\n     \"\"\"\\n     Function takes a dataframe and returns potentially relevant information about it (including a sample)\\n      include=bool, default to False. To add the results from a describe method, pass True to the argument.\\n     samples=int, default to 1. Shows 1 sample by default, but can be modified to include more samples if desired.\\n     \"\"\"\\n          # create the df_inf dataframe\\n     df_inf = pd.DataFrame(index=df.columns,\\n             data = {\\n                 \\'nunique\\':df.nunique()\\n                 ,\\'dtypes\\':df.dtypes\\n                 ,\\'isnull\\':df.isnull().sum()\\n             })\\n          # append samples based on input\\n     if samples >= 1:\\n         df_inf = df_inf.merge(df.sample(samples).iloc[0:samples].T,how=\\'left\\',left_index=True,right_index=True)\\n          # append describe results if option selected\\n     if include == True:\\n         return df_inf.merge(df.describe(include=\\'all\\').T,how=\\'left\\',left_index=True,right_index=True)\\n     elif include == False:\\n         return df_inf\\n     else:\\n         print(\\'Value passed to \"include\" argument is invalid.\\')',\n",
       " 'def check_file_exists(filename,query,url):\\n     \"\"\"\\n     Function takes a filename, query, and url and checks if the file exists. It will load the dataset requested from either SQL or from the local file.\\n     \"\"\"\\n     if os.path.exists(filename):\\n         print(\\'Reading from file...\\')\\n         df = pd.read_csv(filename,index_col=0)\\n     else:\\n         print(\\'Reading from database...\\')\\n         df = pd.read_sql(query,url)\\n                  df.to_csv(filename)\\n          return df',\n",
       " \"def drop_cols(df,cols=[],extras=False,degree=6):\\n     '''\\n     Drops columns. If no columns provided, then returns dataframe as is.\\n          Arguments:\\n     df: Required. DataFrame with columns to be dropped.\\n     cols: List, default is empty. If provided a list, then will drop the columns.\\n     extras: Default is False. If True, will run drop_extras function with provided degree.\\n         drop_extras will use a statistical test to determine a number of categorical columns to be dropped.\\n         Runs after other columns are dropped, which may impact the stats test run.\\n     degree: Default = 6. Used only in case extras is True.\\n     '''\\n     df = df.drop(columns=cols,errors='ignore')\\n          if extras == True:\\n         df = drop_extras(df,degree)\\n              return df\",\n",
       " \"def Xy_sets(tvt_set,target):\\n     '''\\n     Encodes and returns X_sets and y_sets. Takes a list of dataframes(train/validate/test). Iterates through each applying the encode_df function. Then splits the dataframes into the X and y sets.\\n          Requires X_set and y_set.\\n         X_set is a list of 3 dataframes with the target column dropped (should be in train, validate, test order)\\n         y_set is a list of 3 Series containing the target column that was dropped (should be in train, validate, test order)\\n     '''\\n     X_set = []\\n     y_set = []\\n          # encode and split into X and y\\n     # for the sets, 0 = train, 1 = validate, 2 = test (assuming that the tvt_set has been passed in proper order)\\n     for set_ in tvt_set:\\n         encoded_df = encode_df(set_,target)\\n              X_set.append(encoded_df.drop(columns=target))\\n         y_set.append(encoded_df[target])\\n          return X_set,y_set\"]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions = ['encode_df', 'drop_extras','df_info', 'check_file_exists', 'drop_cols', 'Xy_sets']\n",
    "\n",
    "syntax_grabber(functions,base.data_saver(load_function=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee8358-9058-4790-85d2-16816a1cf9fa",
   "metadata": {},
   "source": [
    "### Part 6: Assembly\n",
    "**Big Idea:** Build a file with the desired functions.\n",
    "\n",
    "**Final Product:** A function that will get the filename to be created and the functions to be included from the user. It will then assemble these functions with all their dependencies and write it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3970a29b-2828-485b-badc-e885451e9767",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the filename:  wrangle.py\n"
     ]
    }
   ],
   "source": [
    "# Get the filename from the user\n",
    "filename = get_filename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a579d116-94db-44b1-8f7d-b08e05505e2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wrangle.py'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c91de0a9-24b8-4710-b531-20772d0dccd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current list of entries:\n",
      "df_info: Function takes a dataframe and returns potentially relevant information about it\n",
      "check_file_exists: Generic function to check if a file exists\n",
      "drop_extras: Function to drop extra columns that may have a smaller impact on the model\n",
      "split_categorical: Returns three dataframes split from one for use in model training, validation, and testing\n",
      "drop_cols: Drops columns\n",
      "encode_df: Takes a processed dataframe and encodes the object columns for usage in modeling\n",
      "Xy_sets: Encodes and returns X_sets and y_sets\n",
      "test_hypothesis: Runs a quick statistical test and informs of rejection or failure to reject\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter function names separated by commas:  df_info, check_file_exists, Xy_sets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected functions: df_info, check_file_exists, Xy_sets\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to confirm or 'restart' to start over:  \n"
     ]
    }
   ],
   "source": [
    "# Select for the functions to be included\n",
    "main_functions = select_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ca23a777-5e8b-4724-b0cd-84f980bf2744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_info', 'check_file_exists', 'Xy_sets']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "41c510df-d6f4-474a-bd92-cf27561f18da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the secondary functions that may be required to run\n",
    "primary_functions = find_dependencies(main_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "71ce7ed8-1ef6-4a17-9dbd-33392914bd76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encode_df']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primary_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f15fdb61-a131-4a9e-b660-1338bf9779dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Establish the list of libraries\n",
    "libraries = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c83ee0b9-0549-42e0-b944-7f6fe8885f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the libraries to the set\n",
    "libraries.update(set(find_library_dependencies(main_functions) + find_library_dependencies(primary_functions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a822355c-c839-408d-af74-043c900d0959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'os', 'pandas'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f6e55e37-3406-4269-a65c-cde2134fed75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the library syntaxes\n",
    "libraries_syntax = syntax_grabber(libraries,\n",
    "                                  base.data_saver(load_library=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8ac18985-dbaf-45e0-8ad4-7a169a959b56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import pandas as pd', 'import os']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libraries_syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "005e9056-e089-4ecd-9654-aacafd723d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the function syntaxes\n",
    "function_syntax = syntax_grabber(primary_functions + main_functions,\n",
    "                                base.data_saver(load_function=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4d5d5e4e-4056-4c54-95c2-69758031163f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"def encode_df(df,target):\\n     '''\\n     Takes a processed dataframe and encodes the object columns for usage in modeling.\\n          Takes a dataframe and a target variable (assuming the target variable is an object). Target variable keeps the thing the model is being trained on from splitting and altering it.\\n          !!! MAKE ME MORE DYNAMIC !!!\\n     - Add functionality to check if passed a list or dataframe\\n     - If dataframe, then run standard loop\\n     - If list then check if each item is a dataframe (checking for train/validate/test)\\n     - If list and each item is dataframe, then try loop on each dataframe\\n     - Otherwise return an error\\n     '''\\n     # Get the object columns from the dataframe\\n     obj_col = [col for col in df.columns if df[col].dtype == 'O']\\n          # remove target variable\\n     obj_col.remove(target)\\n          # Begin encoding the object columns\\n     for col in obj_col:\\n         # Grab current column dummies\\n         dummies = pd.get_dummies(df[col],drop_first=True)\\n                  # concatenate the names in a descriptive manner\\n         dummies.columns = [col+'_is_'+column for column in dummies.columns]\\n          # add these new columns to the dataframe\\n         for column in dummies.columns:\\n             df[column] = dummies[column].astype(float)\\n                  # Drop the old columns from the dataframe\\n         df = df.drop(columns=col)\\n          return df \",\n",
       " 'def df_info(df,include=False,samples=1):\\n     \"\"\"\\n     Function takes a dataframe and returns potentially relevant information about it (including a sample)\\n      include=bool, default to False. To add the results from a describe method, pass True to the argument.\\n     samples=int, default to 1. Shows 1 sample by default, but can be modified to include more samples if desired.\\n     \"\"\"\\n          # create the df_inf dataframe\\n     df_inf = pd.DataFrame(index=df.columns,\\n             data = {\\n                 \\'nunique\\':df.nunique()\\n                 ,\\'dtypes\\':df.dtypes\\n                 ,\\'isnull\\':df.isnull().sum()\\n             })\\n          # append samples based on input\\n     if samples >= 1:\\n         df_inf = df_inf.merge(df.sample(samples).iloc[0:samples].T,how=\\'left\\',left_index=True,right_index=True)\\n          # append describe results if option selected\\n     if include == True:\\n         return df_inf.merge(df.describe(include=\\'all\\').T,how=\\'left\\',left_index=True,right_index=True)\\n     elif include == False:\\n         return df_inf\\n     else:\\n         print(\\'Value passed to \"include\" argument is invalid.\\')',\n",
       " 'def check_file_exists(filename,query,url):\\n     \"\"\"\\n     Function takes a filename, query, and url and checks if the file exists. It will load the dataset requested from either SQL or from the local file.\\n     \"\"\"\\n     if os.path.exists(filename):\\n         print(\\'Reading from file...\\')\\n         df = pd.read_csv(filename,index_col=0)\\n     else:\\n         print(\\'Reading from database...\\')\\n         df = pd.read_sql(query,url)\\n                  df.to_csv(filename)\\n          return df',\n",
       " \"def Xy_sets(tvt_set,target):\\n     '''\\n     Encodes and returns X_sets and y_sets. Takes a list of dataframes(train/validate/test). Iterates through each applying the encode_df function. Then splits the dataframes into the X and y sets.\\n          Requires X_set and y_set.\\n         X_set is a list of 3 dataframes with the target column dropped (should be in train, validate, test order)\\n         y_set is a list of 3 Series containing the target column that was dropped (should be in train, validate, test order)\\n     '''\\n     X_set = []\\n     y_set = []\\n          # encode and split into X and y\\n     # for the sets, 0 = train, 1 = validate, 2 = test (assuming that the tvt_set has been passed in proper order)\\n     for set_ in tvt_set:\\n         encoded_df = encode_df(set_,target)\\n              X_set.append(encoded_df.drop(columns=target))\\n         y_set.append(encoded_df[target])\\n          return X_set,y_set\"]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "36d67ad9-6d7b-4ac0-ad25-38ba43eb1d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import os\n"
     ]
    }
   ],
   "source": [
    "for item in libraries_syntax:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4a5cf5d4-2b5b-4f21-bad3-5f5af279de42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def encode_df(df,target):\n",
      "     '''\n",
      "     Takes a processed dataframe and encodes the object columns for usage in modeling.\n",
      "          Takes a dataframe and a target variable (assuming the target variable is an object). Target variable keeps the thing the model is being trained on from splitting and altering it.\n",
      "          !!! MAKE ME MORE DYNAMIC !!!\n",
      "     - Add functionality to check if passed a list or dataframe\n",
      "     - If dataframe, then run standard loop\n",
      "     - If list then check if each item is a dataframe (checking for train/validate/test)\n",
      "     - If list and each item is dataframe, then try loop on each dataframe\n",
      "     - Otherwise return an error\n",
      "     '''\n",
      "     # Get the object columns from the dataframe\n",
      "     obj_col = [col for col in df.columns if df[col].dtype == 'O']\n",
      "          # remove target variable\n",
      "     obj_col.remove(target)\n",
      "          # Begin encoding the object columns\n",
      "     for col in obj_col:\n",
      "         # Grab current column dummies\n",
      "         dummies = pd.get_dummies(df[col],drop_first=True)\n",
      "                  # concatenate the names in a descriptive manner\n",
      "         dummies.columns = [col+'_is_'+column for column in dummies.columns]\n",
      "          # add these new columns to the dataframe\n",
      "         for column in dummies.columns:\n",
      "             df[column] = dummies[column].astype(float)\n",
      "                  # Drop the old columns from the dataframe\n",
      "         df = df.drop(columns=col)\n",
      "          return df \n",
      "\n",
      "def df_info(df,include=False,samples=1):\n",
      "     \"\"\"\n",
      "     Function takes a dataframe and returns potentially relevant information about it (including a sample)\n",
      "      include=bool, default to False. To add the results from a describe method, pass True to the argument.\n",
      "     samples=int, default to 1. Shows 1 sample by default, but can be modified to include more samples if desired.\n",
      "     \"\"\"\n",
      "          # create the df_inf dataframe\n",
      "     df_inf = pd.DataFrame(index=df.columns,\n",
      "             data = {\n",
      "                 'nunique':df.nunique()\n",
      "                 ,'dtypes':df.dtypes\n",
      "                 ,'isnull':df.isnull().sum()\n",
      "             })\n",
      "          # append samples based on input\n",
      "     if samples >= 1:\n",
      "         df_inf = df_inf.merge(df.sample(samples).iloc[0:samples].T,how='left',left_index=True,right_index=True)\n",
      "          # append describe results if option selected\n",
      "     if include == True:\n",
      "         return df_inf.merge(df.describe(include='all').T,how='left',left_index=True,right_index=True)\n",
      "     elif include == False:\n",
      "         return df_inf\n",
      "     else:\n",
      "         print('Value passed to \"include\" argument is invalid.')\n",
      "\n",
      "def check_file_exists(filename,query,url):\n",
      "     \"\"\"\n",
      "     Function takes a filename, query, and url and checks if the file exists. It will load the dataset requested from either SQL or from the local file.\n",
      "     \"\"\"\n",
      "     if os.path.exists(filename):\n",
      "         print('Reading from file...')\n",
      "         df = pd.read_csv(filename,index_col=0)\n",
      "     else:\n",
      "         print('Reading from database...')\n",
      "         df = pd.read_sql(query,url)\n",
      "                  df.to_csv(filename)\n",
      "          return df\n",
      "\n",
      "def Xy_sets(tvt_set,target):\n",
      "     '''\n",
      "     Encodes and returns X_sets and y_sets. Takes a list of dataframes(train/validate/test). Iterates through each applying the encode_df function. Then splits the dataframes into the X and y sets.\n",
      "          Requires X_set and y_set.\n",
      "         X_set is a list of 3 dataframes with the target column dropped (should be in train, validate, test order)\n",
      "         y_set is a list of 3 Series containing the target column that was dropped (should be in train, validate, test order)\n",
      "     '''\n",
      "     X_set = []\n",
      "     y_set = []\n",
      "          # encode and split into X and y\n",
      "     # for the sets, 0 = train, 1 = validate, 2 = test (assuming that the tvt_set has been passed in proper order)\n",
      "     for set_ in tvt_set:\n",
      "         encoded_df = encode_df(set_,target)\n",
      "              X_set.append(encoded_df.drop(columns=target))\n",
      "         y_set.append(encoded_df[target])\n",
      "          return X_set,y_set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in function_syntax:\n",
    "    print(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4e0515a5-1791-4a4f-89fa-6414c20c23f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def assembler():\n",
    "    # Get the filename from the user\n",
    "    filename = get_filename()\n",
    "    \n",
    "    # Select for the functions to be included\n",
    "    main_functions = select_functions()\n",
    "    \n",
    "    # Get the secondary functions that may be required to run\n",
    "    primary_functions = find_dependencies(main_functions)\n",
    "    \n",
    "    # Establish the list of libraries\n",
    "    libraries = set()\n",
    "    \n",
    "    # Add the libraries to the set\n",
    "    libraries.update(set(find_library_dependencies(main_functions) + find_library_dependencies(primary_functions)))\n",
    "    \n",
    "    # Retrieve the library syntaxes\n",
    "    libraries_syntax = syntax_grabber(libraries,\n",
    "                                      base.data_saver(load_library=True))\n",
    "    \n",
    "    # Retrieve the function syntaxes\n",
    "    function_syntax = syntax_grabber(primary_functions + main_functions,\n",
    "                                    base.data_saver(load_function=True))\n",
    "    \n",
    "    # Build the file\n",
    "    print(f'Creating {filename}...')\n",
    "    with open(filename,'w') as file:\n",
    "        for item in libraries_syntax:\n",
    "            file.write(item + '\\n')\n",
    "            \n",
    "        file.write('\\n')\n",
    "        \n",
    "        for item in function_syntax:\n",
    "            file.write(item + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2410d2f1-9c1b-45a2-afa7-a1e31788704b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current list of entries:\n",
      "df_info: Function takes a dataframe and returns potentially relevant information about it\n",
      "check_file_exists: Generic function to check if a file exists\n",
      "drop_extras: Function to drop extra columns that may have a smaller impact on the model\n",
      "split_categorical: Returns three dataframes split from one for use in model training, validation, and testing\n",
      "drop_cols: Drops columns\n",
      "encode_df: Takes a processed dataframe and encodes the object columns for usage in modeling\n",
      "Xy_sets: Encodes and returns X_sets and y_sets\n",
      "test_hypothesis: Runs a quick statistical test and informs of rejection or failure to reject\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter function names separated by commas:  df_info, check_file_exists, split_categorical, Xy_sets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected functions: df_info, check_file_exists, split_categorical, Xy_sets\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to confirm or 'restart' to start over:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating wrangle.py...\n"
     ]
    }
   ],
   "source": [
    "assembler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d9017-7cb4-4d9e-9ceb-1e384e51efb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
